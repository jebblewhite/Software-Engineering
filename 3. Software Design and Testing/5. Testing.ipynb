{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During these notebooks, we have been seeing how to add maintain your code clean and your functions and classes as flexible as possible. And maybe adding more feature to a flexible code can cause problems. Can we be sure that by adding more feature to a flexible code, the existing functions and classes are going to do what they were intended to do?\n",
    "\n",
    "We need to make sure our code live on long into the future, so we need some means to assure its long-term stability. __Testing__ is one of the most important tools to do that.\n",
    "\n",
    "Testing is a thorough and formal process for applications that must not fail. It, in essence, consists on verifying that a program works as intended.\n",
    "\n",
    "> ## Testing is the process of verifying that a software behaves the way you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason to test a software is to determine if it does what it claims to do. Imagine you are using a function, and it doesn't raise an error. However, the output is not what you expected, that mistake will cascade as the application progresses. \n",
    "\n",
    "In general terms we can divide testing into two categories: Functional and Non-Functional. In this notebook we will focus on Functional Testing, so we will start explainin Non-Functional Testing first, so we can focus our attention on the Functional Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-functional testing\n",
    "\n",
    "> __Testing various components of the systems not directly related with desired functionality__\n",
    "\n",
    "### Performance testing\n",
    "\n",
    "> __How performant our product is under different conditions__\n",
    "\n",
    "During those tests there are a few key things to keep in mind:\n",
    "- __Find bottlenecks__ - where your code takes absurdly long to run (maybe it is a single slow operation you can change?)\n",
    "- __Premature optimization is the root of all evil__ - if it is fast enough, don't try to improve by `0.1%`\n",
    "- __Test under different load__, some examples:\n",
    "    - How the performance differs based on increased batch size?\n",
    "    - __Spike testing__: What if our machine learning app deployed on AWS has a sudden user spike?\n",
    "    - __Stress testing__: how your product behaves at __or even above__ it's limits (e.g. large input values, large data, large traffic), is this how you envisioned it?\n",
    "    - __Endurance testing__: normal load but for a long time; how often is your web app down?\n",
    "    \n",
    "    \n",
    "### Security testing\n",
    "\n",
    "> __Keep in mind this topic is way too broad and worthy of another course on it's own!__\n",
    "\n",
    "Importance of this topic is often underestimated, but it is an essential piece of many infrastructures.\n",
    "Few things you should keep in mind:\n",
    "- __Minimum trust approach__ - give only absolutely necessary permissions to users/coworkers\n",
    "- __Separate roles__ - permissions only related to their roles\n",
    "- __Try to break it__ - check out pentesting or ethical hacking\n",
    "\n",
    "### Compatibility testing\n",
    "\n",
    "> __How compatible is our product with previous iteration and/or different environments__\n",
    "\n",
    "Luckily the second type of compatibility can be simply improved by using `docker` (__principle of shifting responsibility to providers__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other helpful techniques\n",
    "\n",
    "> In order to keep your code in check one can employ a few simple additional techniques\n",
    "\n",
    "- __Peer review__ - each thing you do is checked by another person:\n",
    "    - Pull Requests are often checked by assigned reviewers\n",
    "    - Scientific papers are under double blind peer-review\n",
    "- __Code analyzers__ - GitHub offers a lot of integrations, __which looks for possible bugs in your code automatically__, a few examples with easy integration:\n",
    "    - [`codebeat`](https://codebeat.co/)\n",
    "    - [`sonarqube`](https://www.sonarqube.org/)\n",
    "    - [`codacy`](https://www.codacy.com/)\n",
    "    - [`codeclimate`](https://codeclimate.com/) \n",
    "- __Test coverage__ - how many (in percentage) of our code was tested. One can obtain it via [`coverage.py`](https://coverage.readthedocs.io/en/coverage-5.5/) with testing framework of choice (also it is possible to integrate with GitHub Actions, which we will see in a few lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Testing\n",
    "\n",
    "_Functional Testing_ consists on making sure that a piece of code _functions_ correctly. We will see other type of testing later. The basic structure of a functional test is:\n",
    "1. Prepare the inputs to the software.\n",
    "2. Identify the outputs of the software.\n",
    "3. Run the software with the inputs and check the outputs.\n",
    "4. Compare the outputs with the expected outputs to see if they match. \n",
    "\n",
    "The two first steps are performed by the developer or author of the software. The third step is done by the software itself. And the last step is done by a tester (we will see testers later).\n",
    "\n",
    "![](images/functional_testing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say for example that we want to check the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_list(my_list: list) -> float:\n",
    "    \"\"\"\n",
    "    Return the mean of the values in the list.\n",
    "    \"\"\"\n",
    "    running_sum = 0\n",
    "    for num in my_list:\n",
    "        running_sum += num\n",
    "    return running_sum / len(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the list `[1, 2, 3]` should return a mean of 2, the list [1, 2, 3, 4] should return a mean of 2.5, and the list [1, 2, 3, 4, 5] should return a mean of 3...\n",
    "\n",
    "We can create some manual tests to check that the output is what we are expecting. This is named _Manual Testing_, and it is a good idea to do it before you write any code. However, as you start progress, you won't be able to manually test every possible case (due to the time it would take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f7c4a06be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2e4729877de6>\u001b[0m in \u001b[0;36mmean_list\u001b[0;34m(my_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrunning_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "assert mean_list([1, 2, 3]) == 2\n",
    "assert mean_list([-1, -2, 1, 2]) == 0\n",
    "assert mean_list([]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we might rely on _Automatic Testing_, which consists on writing a great amount of tests that can then be executed as many times as wanted. Automated tests will eventually discover things to be fixed, so you should modify your code to make the testing, and therefore, your code, more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance Testing\n",
    "These tests, as you might notice are testing small pieces of code. However we will need to test the code as a whole. This is called __Acceptance Testing__, which is often performed by business stakeholders. They can also be autommated using _end-to-end testing_ to make sure that a list of actions are carried out.\n",
    "\n",
    "As you can see, __Acceptance Testing__ is not very granular, and in fact, is the least granular of all the testing techniques. At a lower level of granularity we find __System Testing__\n",
    "### System Testing\n",
    "System Testing is a testing technique that is used to test the entire system. This is done by testing the entire system from the very beginning and then adding features one by one.\n",
    "### Integration Testing\n",
    "System Testing is still not very granular, since it takes the whole system. Integration Testing is a testing technique that is used to test the interconnection between different parts of the system. This is done by testing parts of the code framed as scenarios.\n",
    "### Unit Testing\n",
    "The lower level of granularity is __Unit Testing__. Unit Testing is a testing technique that is used to test individual units of code. It is usually done by the developers themselves, and it consists on testing specific functions/methods to see that they return assumed values/do assumed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyramid Testing\n",
    "\n",
    "Let's recap the type of testing we've seen so far:\n",
    "\n",
    "![](images/pyramid_testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that unit test is the basis of testing design. Thus, in this notebook we will focus on this type of testing using the unittest module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Testing\n",
    "\n",
    "Regression testing is not a testing per se, but rather an approach to develop your application. Regression testing consists on adding tests to your collection as you develop your application and find more and more bugs. This collection is named _test suite_, and developers run them in a _continuous integration_ (CI) server. Some of the most famous CI servers are [Jenkins](https://www.jenkins.io), [Travis CI](https://travis-ci.org) or [CircleCI](https://circleci.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessments:\n",
    "\n",
    "## 1. Look for Behaviour-Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}