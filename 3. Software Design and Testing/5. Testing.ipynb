{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During these notebooks, we have been seeing how to add maintain your code clean and your functions and classes as flexible as possible. And maybe adding more feature to a flexible code can cause problems. Can we be sure that by adding more feature to a flexible code, the existing functions and classes are going to do what they were intended to do?\n",
    "\n",
    "We need to make sure our code live on long into the future, so we need some means to assure its long-term stability. __Testing__ is one of the most important tools to do that.\n",
    "\n",
    "Testing is a thorough and formal process for applications that must not fail. It, in essence, consists on verifying that a program works as intended.\n",
    "\n",
    "> ## Testing is the process of verifying that a software behaves the way you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason to test a software is to determine if it does what it claims to do. Imagine you are using a function, and it doesn't raise an error. However, the output is not what you expected, that mistake will cascade as the application progresses. \n",
    "\n",
    "In general terms we can divide testing into two categories: Functional and Non-Functional. In this notebook we will focus on Functional Testing, so we will start explainin Non-Functional Testing first, so we can focus our attention on the Functional Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-functional testing\n",
    "\n",
    "> __Testing various components of the systems not directly related with desired functionality__\n",
    "\n",
    "### Performance testing\n",
    "\n",
    "> __How performant our product is under different conditions__\n",
    "\n",
    "During those tests there are a few key things to keep in mind:\n",
    "- __Find bottlenecks__ - where your code takes absurdly long to run (maybe it is a single slow operation you can change?)\n",
    "- __Premature optimization is the root of all evil__ - if it is fast enough, don't try to improve by `0.1%`\n",
    "- __Test under different load__, some examples:\n",
    "    - How the performance differs based on increased batch size?\n",
    "    - __Spike testing__: What if our machine learning app deployed on AWS has a sudden user spike?\n",
    "    - __Stress testing__: how your product behaves at __or even above__ it's limits (e.g. large input values, large data, large traffic), is this how you envisioned it?\n",
    "    - __Endurance testing__: normal load but for a long time; how often is your web app down?\n",
    "    \n",
    "    \n",
    "### Security testing\n",
    "\n",
    "> __Keep in mind this topic is way too broad and worthy of another course on it's own!__\n",
    "\n",
    "Importance of this topic is often underestimated, but it is an essential piece of many infrastructures.\n",
    "Few things you should keep in mind:\n",
    "- __Minimum trust approach__ - give only absolutely necessary permissions to users/coworkers\n",
    "- __Separate roles__ - permissions only related to their roles\n",
    "- __Try to break it__ - check out pentesting or ethical hacking\n",
    "\n",
    "### Compatibility testing\n",
    "\n",
    "> __How compatible is our product with previous iteration and/or different environments__\n",
    "\n",
    "Luckily the second type of compatibility can be simply improved by using `docker` (__principle of shifting responsibility to providers__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other helpful techniques\n",
    "\n",
    "> In order to keep your code in check one can employ a few simple additional techniques\n",
    "\n",
    "- __Peer review__ - each thing you do is checked by another person:\n",
    "    - Pull Requests are often checked by assigned reviewers\n",
    "    - Scientific papers are under double blind peer-review\n",
    "- __Code analyzers__ - GitHub offers a lot of integrations, __which looks for possible bugs in your code automatically__, a few examples with easy integration:\n",
    "    - [`codebeat`](https://codebeat.co/)\n",
    "    - [`sonarqube`](https://www.sonarqube.org/)\n",
    "    - [`codacy`](https://www.codacy.com/)\n",
    "    - [`codeclimate`](https://codeclimate.com/) \n",
    "- __Test coverage__ - how many (in percentage) of our code was tested. One can obtain it via [`coverage.py`](https://coverage.readthedocs.io/en/coverage-5.5/) with testing framework of choice (also it is possible to integrate with GitHub Actions, which we will see in a few lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Testing\n",
    "\n",
    "_Functional Testing_ consists on making sure that a piece of code _functions_ correctly. We will see other type of testing later. The basic structure of a functional test is:\n",
    "1. Prepare the inputs to the software.\n",
    "2. Identify the outputs of the software.\n",
    "3. Run the software with the inputs and check the outputs.\n",
    "4. Compare the outputs with the expected outputs to see if they match. \n",
    "\n",
    "The two first steps are performed by the developer or author of the software. The third step is done by the software itself. And the last step is done by a tester (we will see testers later).\n",
    "\n",
    "![](images/functional_testing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say for example that we want to check the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_list(my_list: list) -> float:\n",
    "    \"\"\"\n",
    "    Return the mean of the values in the list.\n",
    "    \"\"\"\n",
    "    running_sum = 0\n",
    "    for num in my_list:\n",
    "        running_sum += num\n",
    "    return running_sum / len(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the list `[1, 2, 3]` should return a mean of 2, the list [1, 2, 3, 4] should return a mean of 2.5, and the list [1, 2, 3, 4, 5] should return a mean of 3...\n",
    "\n",
    "We can create some manual tests to check that the output is what we are expecting. This is named _Manual Testing_, and it is a good idea to do it before you write any code. However, as you start progress, you won't be able to manually test every possible case (due to the time it would take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f7c4a06be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2e4729877de6>\u001b[0m in \u001b[0;36mmean_list\u001b[0;34m(my_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrunning_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "assert mean_list([1, 2, 3]) == 2\n",
    "assert mean_list([-1, -2, 1, 2]) == 0\n",
    "assert mean_list([]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we might rely on _Automatic Testing_, which consists on writing a great amount of tests that can then be executed as many times as wanted. Automated tests will eventually discover things to be fixed, so you should modify your code to make the testing, and therefore, your code, more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance Testing\n",
    "These tests, as you might notice are testing small pieces of code. However we will need to test the code as a whole. This is called __Acceptance Testing__, which is often performed by business stakeholders. They can also be autommated using _end-to-end testing_ to make sure that a list of actions are carried out.\n",
    "\n",
    "As you can see, __Acceptance Testing__ is not very granular, and in fact, is the least granular of all the testing techniques. At a lower level of granularity we find __System Testing__\n",
    "### System Testing\n",
    "System Testing is a testing technique that is used to test the entire system. This is done by testing the entire system from the very beginning and then adding features one by one.\n",
    "### Integration Testing\n",
    "System Testing is still not very granular, since it takes the whole system. Integration Testing is a testing technique that is used to test the interconnection between different parts of the system. This is done by testing parts of the code framed as scenarios.\n",
    "### Unit Testing\n",
    "The lower level of granularity is __Unit Testing__. Unit Testing is a testing technique that is used to test individual units of code. It is usually done by the developers themselves, and it consists on testing specific functions/methods to see that they return assumed values/do assumed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyramid Testing\n",
    "\n",
    "Let's recap the type of testing we've seen so far:\n",
    "\n",
    "![](images/pyramid_testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that unit test is the basis of testing design. Thus, in this notebook we will focus on this type of testing using the unittest module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Testing\n",
    "\n",
    "Regression testing is not a testing per se, but rather an approach to develop your application. Regression testing consists on adding tests to your collection as you develop your application and find more and more bugs. This collection is named _test suite_, and developers run them in a _continuous integration_ (CI) server. Some of the most famous CI servers are [Jenkins](https://www.jenkins.io), [Travis CI](https://travis-ci.org) or [CircleCI](https://circleci.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## A Unit is a small, fundamental piece of software\n",
    "\n",
    "Unit testing seels to verify that all the individual units of code in your application work correctly. We can implement unit testing using the `unittest` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Unit Testing using `unittest`\n",
    "\n",
    "Unittest is Python's built-in testing framework. Despite its name, it can also be used for integration testing. The module provides features for making assertions about the behaviour of code, and for comparing the behaviour of code to a known result. It also includes the tool for running tests.\n",
    "\n",
    "To start testing your code using unittest, you need to create a class that inherits from `unittest.TestCase`. By convention, the name of the class should have this syntax:\n",
    "```\n",
    "class <Object>TestCase(unittest.TestCase):\n",
    "```\n",
    "Where `<Object>` is the name of the class you want to test. For example, if you are testing a scraper, the name of the class would be `ScraperTestCase`. In this case, we are going to test the Date class in Date.py\n",
    "\n",
    "This class will have methods, and each method will assert a certain aspect of the behaviour of the class. For example, we will test the `__str__` method, which should return a string representation of the date.\n",
    "\n",
    "Unittest will include assertion methods that you can use to compare the output with the expected output. It will also include decorators to skip tests if a condition is met.\n",
    "\n",
    "The final step will be running the test. To do this, we need to import the `unittest` module, and then, we can run the test in the script using the `unittest.main()` function, or in the command line using the `python -m unittest` command. The latter command is the most common way to run tests in Python, but the first one is useful when developing a script. Thus, in this notebook, we will see how to perform both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Test class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we test something, we need something to test! Luckily, we have a scrip named `product.py` that we can use to test. Take a look at that script and try to figure out what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 0 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fa90918fdf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from examples.product import Product\n",
    "import unittest\n",
    "\n",
    "class TestProduct(unittest.TestCase):\n",
    "    pass\n",
    "# unittest.main looks at sys.argv by default, which is what started IPython, hence the error about the kernel connection file not being a valid attribute. You can pass an explicit list to main to avoid looking up sys.argv.\n",
    "# In the notebook, you will also want to include exit=False to prevent unittest.main from trying to shutdown the kernel process:\n",
    "# If you are using a script, you don't need to pass any argument to main.\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report says that there are no errors in the code, which makes sense, because we don't have any test.\n",
    "As mentioned we can use the `unittest` moduel in the Command Line to test our code. To do this, we need to test a script whose name starts with `test_` (we can actually change that convention, but it's not recommended).\n",
    "\n",
    "Go create a new file called `test_product.py`. We will see how to organize the tests in the next notebook (the infamous Notebook 6). In the file include the same ProductTestCase:\n",
    "\n",
    "```\n",
    "from examples.product import Product\n",
    "import unittest\n",
    "\n",
    "class ProductTestCase(unittest.TestCase):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in the command line type `python -m unittest`. This will detect all modules whose name start with test, and inside there, it will look at tests established in the class. The output you will see now is the same as in the cell:\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------------\n",
    "Ran 0 tests in 0.000s\n",
    "\n",
    "OK\n",
    "```\n",
    "\n",
    "Which makes sense, because we haven't programmed any test yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's populate the ProductTestCase. When we run a unit test, the module will check the assertions included in the methods. Take into account that an error due to other issue (for example a syntax error) will not count as a failed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_transform_name (__main__.TestProduct) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.040s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f8b4ecb5a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from examples.product import Product\n",
    "import unittest\n",
    "\n",
    "class TestProduct(unittest.TestCase):\n",
    "    def test_transform_name(self):\n",
    "        small_black_shoes = Product('shoes', 'S', 'black')\n",
    "        expected_value = 'SHOES'\n",
    "        actual_value = small_black_shoes.transform_name_for_sku()\n",
    "        self.assertEqual(expected_value, actual_value)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the name of the method to something that doesn't contain test at the beginning\n",
    "\n",
    "Try changing the expected answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessments:\n",
    "\n",
    "## 1. Look for Behaviour-Driven Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}