{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "During these notebooks, we have been seeing how to add maintain your code clean and your functions and classes as flexible as possible. And maybe adding more feature to a flexible code can cause problems. Can we be sure that by adding more feature to a flexible code, the existing functions and classes are going to do what they were intended to do?\n",
    "\n",
    "We need to make sure our code live on long into the future, so we need some means to assure its long-term stability. __Testing__ is one of the most important tools to do that.\n",
    "\n",
    "Testing is a thorough and formal process for applications that must not fail. It, in essence, consists on verifying that a program works as intended.\n",
    "\n",
    "> ## Testing is the process of verifying that a software behaves the way you expect."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One reason to test a software is to determine if it does what it claims to do. Imagine you are using a function, and it doesn't raise an error. However, the output is not what you expected, that mistake will cascade as the application progresses. \n",
    "\n",
    "In general terms we can divide testing into two categories: Functional and Non-Functional. In this notebook we will focus on Functional Testing, so we will start explainin Non-Functional Testing first, so we can focus our attention on the Functional Testing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non-functional testing\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> __Testing various components of the systems not directly related with desired functionality__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Performance testing\n",
    "\n",
    "> __How performant our product is under different conditions__\n",
    "\n",
    "During those tests there are a few key things to keep in mind:\n",
    "- __Find bottlenecks__ - where your code takes absurdly long to run (maybe it is a single slow operation you can change?)\n",
    "- __Premature optimization is the root of all evil__ - if it is fast enough, don't try to improve by `0.1%`\n",
    "- __Test under different load__, some examples:\n",
    "    - How the performance differs based on increased batch size?\n",
    "    - __Spike testing__: What if our machine learning app deployed on AWS has a sudden user spike?\n",
    "    - __Stress testing__: how your product behaves at __or even above__ it's limits (e.g. large input values, large data, large traffic), is this how you envisioned it?\n",
    "    - __Endurance testing__: normal load but for a long time; how often is your web app down?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Security testing\n",
    "\n",
    "> __Keep in mind this topic is way too broad and worthy of another course on it's own!__\n",
    "\n",
    "Importance of this topic is often underestimated, but it is an essential piece of many infrastructures.\n",
    "Few things you should keep in mind:\n",
    "- __Minimum trust approach__ - give only absolutely necessary permissions to users/coworkers\n",
    "- __Separate roles__ - permissions only related to their roles\n",
    "- __Try to break it__ - check out pentesting or ethical hacking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Compatibility testing\n",
    "\n",
    "> __How compatible is our product with previous iteration and/or different environments__\n",
    "\n",
    "Luckily the second type of compatibility can be simply improved by using `docker` (__principle of shifting responsibility to providers__)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other helpful techniques\n",
    "\n",
    "> In order to keep your code in check one can employ a few simple additional techniques\n",
    "\n",
    "- __Peer review__ - each thing you do is checked by another person:\n",
    "    - Pull Requests are often checked by assigned reviewers\n",
    "    - Scientific papers are under double blind peer-review\n",
    "- __Code analyzers__ - GitHub offers a lot of integrations, __which looks for possible bugs in your code automatically__, a few examples with easy integration:\n",
    "    - [`codebeat`](https://codebeat.co/)\n",
    "    - [`sonarqube`](https://www.sonarqube.org/)\n",
    "    - [`codacy`](https://www.codacy.com/)\n",
    "    - [`codeclimate`](https://codeclimate.com/) \n",
    "- __Test coverage__ - how many (in percentage) of our code was tested. One can obtain it via [`coverage.py`](https://coverage.readthedocs.io/en/coverage-5.5/) with testing framework of choice (also it is possible to integrate with GitHub Actions, which we will see in a few lessons)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functional Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "_Functional Testing_ consists on making sure that a piece of code _functions_ correctly. The basic structure of a functional test is:\n",
    "1. Prepare the inputs to the software.\n",
    "2. Identify the outputs of the software.\n",
    "3. Run the software with the inputs and check the outputs.\n",
    "4. Compare the outputs with the expected outputs to see if they match. \n",
    "\n",
    "The two first steps are performed by the developer or author of the software. The third step is done by the software itself. And the last step is done by a tester (we will see testers later).\n",
    "\n",
    "![](images/functional_testing.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's say for example that we want to check the mean of a list of numbers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def mean_list(my_list: list) -> float:\r\n",
    "    \"\"\"\r\n",
    "    Return the mean of the values in the list.\r\n",
    "    \"\"\"\r\n",
    "    running_sum = 0\r\n",
    "    for num in my_list:\r\n",
    "        running_sum += num\r\n",
    "    return running_sum / len(my_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the list `[1, 2, 3]` should return a mean of 2, the list [1, 2, 3, 4] should return a mean of 2.5, and the list [1, 2, 3, 4, 5] should return a mean of 3...\n",
    "\n",
    "We can create some manual tests to check that the output is what we are expecting. This is named _Manual Testing_, and it is a good idea to do it before you write any code. However, as you start progress, you won't be able to manually test every possible case (due to the time it would take)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "assert mean_list([1, 2, 3]) == 2\r\n",
    "assert mean_list([-1, -2, 1, 2]) == 0\r\n",
    "assert mean_list([]) == 0\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f7c4a06be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2e4729877de6>\u001b[0m in \u001b[0;36mmean_list\u001b[0;34m(my_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrunning_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, we might rely on _Automatic Testing_, which consists on writing a great amount of tests that can then be executed as many times as wanted. Automated tests will eventually discover things to be fixed, so you should modify your code to make the testing, and therefore, your code, more robust."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acceptance Testing\r\n",
    "These tests, as you might notice are testing small pieces of code. However we will need to test the code as a whole. This is called __Acceptance Testing__, which is often performed by business stakeholders. They can also be autommated using _end-to-end testing_ to make sure that a list of actions are carried out.\r\n",
    "\r\n",
    "> Acceptance testing: Is the API satisfying the stakeholders needs?\r\n",
    "\r\n",
    "As you can see, __Acceptance Testing__ is not very granular, and in fact, is the least granular of all the testing techniques. \r\n",
    "\r\n",
    "At a lower level of granularity we find __System Testing__\r\n",
    "## System Testing\r\n",
    "System Testing is a testing technique that is used to test the entire system. This is done by testing the entire system from the very beginning and then adding features one by one.\r\n",
    "\r\n",
    "> System Testing: Is the API available and performing as expected?\r\n",
    "\r\n",
    "System Testing is still not very granular, since it takes the whole system\r\n",
    "\r\n",
    "## Integration Testing\r\n",
    "Integration Testing is a testing technique that is used to test the interconnection between different parts of the system. This is done by testing parts of the code framed as scenarios.\r\n",
    "\r\n",
    "> Integration Testing: Is every endpoint working as expected?\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "## Unit Testing\r\n",
    "The lower level of granularity is __Unit Testing__. Unit Testing is a testing technique that is used to test individual units of code. It is usually done by the developers themselves, and it consists on testing specific functions/methods to see that they return assumed values/do assumed things\r\n",
    "\r\n",
    "> Unit Testing: Is each code module working properly?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pyramid Testing\n",
    "\n",
    "Let's recap the type of testing we've seen so far:\n",
    "\n",
    "![](images/pyramid_testing.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that unit test is the basis of testing design. Thus, in this notebook we will focus on this type of testing using the unittest module."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression Testing\n",
    "\n",
    "Regression testing is not a testing per se, but rather an approach to develop your application. Regression testing consists on adding tests to your collection as you develop your application and find more and more bugs. This collection is named _test suite_, and developers run them in a _continuous integration_ (CI) server. Some of the most famous CI servers are [Jenkins](https://www.jenkins.io), [Travis CI](https://travis-ci.org) or [CircleCI](https://circleci.com)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unit Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## A Unit is a small, fundamental piece of software\n",
    "\n",
    "Unit testing seeks to verify that all the individual units of code in your application work correctly. We can implement unit testing using the `unittest` module.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Unit Testing using `unittest`\n",
    "\n",
    "Unittest is Python's built-in testing framework. Despite its name, it can also be used for integration testing. The module provides features for making assertions about the behaviour of code, and for comparing the behaviour of code to a known result. It also includes the tool for running tests.\n",
    "\n",
    "To start testing your code using unittest, you need to create a class that inherits from `unittest.TestCase`. By convention, the name of the class should have this syntax:\n",
    "```\n",
    "class <Object>TestCase(unittest.TestCase):\n",
    "```\n",
    "Where `<Object>` is the name of the class or function you want to test. For example, if you are testing a scraper, the name of the class would be `ScraperTestCase`. In this case, we are going to test the Date class in Date.py\n",
    "\n",
    "This class will have methods, and each method will assert a certain aspect of the behaviour of the class. For example, you can test the `__str__` method, which should return a string representation of the date.\n",
    "\n",
    "Unittest will include assertion methods that you can use to compare the output with the expected output. It will also include decorators to skip tests if a condition is met.\n",
    "\n",
    "The final step will be running the test. To do this, we need to import the `unittest` module, and then, we can run the test in the script using the `unittest.main()` function, or in the command line using the `python -m unittest` command. The latter command is the most common way to run tests in Python, but the first one is useful when developing a script. Thus, in this notebook, we will see how to perform both"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Test class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we test something, we need something to test! Luckily, we have a script named `product.py` that we can use to test. Take a look at that script and try to figure out what it does."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from example.product import Product\r\n",
    "import unittest\r\n",
    "\r\n",
    "class ProductTestCase(unittest.TestCase):\r\n",
    "    pass\r\n",
    "# unittest.main looks at sys.argv by default, which is what started IPython, hence the error about the kernel connection file not being a valid attribute. You can pass an explicit list to main to avoid looking up sys.argv.\r\n",
    "# In the notebook, you will also want to include exit=False to prevent unittest.main from trying to shutdown the kernel process:\r\n",
    "# If you are using a script, you don't need to pass any argument to main.\r\n",
    "\r\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 0 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f948f3775e0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The report says that there are no errors in the code, which makes sense, because we don't have any!\n",
    "\n",
    "As mentioned, we can use the `unittest` module in the Command Line to test our code. To do this, we need to test a script whose name starts with `test_` (we can actually change that convention, but it's not recommended).\n",
    "\n",
    "Go create a new file called `test_product.py`. We will see how to organize the tests in the next notebook (the infamous Notebook 6). In the file include the same ProductTestCase:\n",
    "\n",
    "```\n",
    "from Cart.product import Product\n",
    "import unittest\n",
    "\n",
    "class ProductTestCase(unittest.TestCase):\n",
    "    pass\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, in the command line type `python -m unittest`. This will detect all modules whose name start with test, and inside there, it will look at tests established in the class. The output you will see now is the same as in the cell:\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------------\n",
    "Ran 0 tests in 0.000s\n",
    "\n",
    "OK\n",
    "```\n",
    "\n",
    "Which makes sense, because we haven't programmed any test yet."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your first unit test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's populate the ProductTestCase. When we run a unit test, the module will check the assertions included in the methods. Take into account that an error due to other issue (for example a syntax error) will not count as a failed test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from example.product import Product\r\n",
    "import unittest\r\n",
    "\r\n",
    "class ProductTestCase(unittest.TestCase):\r\n",
    "    def test_transform_name(self):\r\n",
    "        small_black_shoes = Product('shoes', 'S', 'black')\r\n",
    "        expected_value = 'SHOES'\r\n",
    "        actual_value = small_black_shoes.transform_name_for_sku()\r\n",
    "        self.assertEqual(expected_value, actual_value)\r\n",
    "\r\n",
    "unittest.main(argv=[''], verbosity=0, exit=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f948f25f550>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This is a big font\n",
    "## This is a smaller one"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try it yourself (20 min)\n",
    "\n",
    "- Try changing the name of the method to something that doesn't contain test at the beginning\n",
    "\n",
    "    - Add the 'incorrect' function to the `test_product.py` script.\n",
    "    - Run unittest on the command line <br>\n",
    "        <br>\n",
    "        - Go back to the 'right' function and try changing the expected value\n",
    "            - Add the 'right' function to the `test_product.py` script.\n",
    "            - Run unittest on the command line<br>\n",
    "        <br>\n",
    "        - Change functionality to the `transform_name_for_sku()` method:\n",
    "            - It will now Capitalize the name of the product instead of UPPERCASING it\n",
    "            - Run the same test again\n",
    "            - Observe the errors\n",
    "            - This will happen every time you make a slight change to your code, and it might be annoying changing the test every time. But imagine what will happen when you start adding more and more code. A slight change can cause a huge snowball effect!<br>\n",
    "        <br>\n",
    "        - Add two more test methods:\n",
    "            - test_transform_color_for_sku, which asserts that the colour is well written after using `transform_color_for_sku()`\n",
    "            - test_generate_sku, which asserts that the SKU is well written after using `generate_sku()`\n",
    "            - for both cases, use `Product('shoes', 'S', 'black')`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setUp() and tearDown()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the last part of the exercise, you had to set the same product `Product('shoes', 'S', 'black')` twice, and added to the already existing product in `test_transform_name_for_skull`, that's three times. Is there a way to use the same scenario for each test? Well, of course there is, otherwise, I wouldn't be writting this. You can do it using the method setUp(), which will run at the beginning of each test. The opposite of setUp() is tearDown(), which is run everytime the test finishes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from example.product import Product\r\n",
    "import unittest\r\n",
    "\r\n",
    "class ProductTestCase(unittest.TestCase):\r\n",
    "    # Initialize the scenario for your test\r\n",
    "    def setUp(self):\r\n",
    "        self.product = Product('shoes', 'S', 'black')\r\n",
    "    def test_transform_name(self):\r\n",
    "        expected_value = 'SHOES'\r\n",
    "        actual_value = self.product.transform_name_for_sku()\r\n",
    "        self.assertEqual(expected_value, actual_value)\r\n",
    "    # Finish \r\n",
    "    def tearDown(self):\r\n",
    "        del self.product\r\n",
    "\r\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_add_and_remove_product (__main__.ShoppingCartTestCase) ... ok\n",
      "test_transform_name (__main__.TestProduct) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.024s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fec11718df0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case tearDown is not very useful, because we don't care about product once we finish the test anyway. Another example is using setUp and tearDown to open and close a file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import unittest\r\n",
    "\r\n",
    "\r\n",
    "class FileTestCase(unittest.TestCase):\r\n",
    "    def setUp(self):\r\n",
    "        self.handle = open(\"Cart/product.py\", \"r\")\r\n",
    "\r\n",
    "    def test_length(self):\r\n",
    "        length = len(self.handle.readlines())\r\n",
    "        self.assertTrue(length > 20)\r\n",
    "\r\n",
    "    def tearDown(self):\r\n",
    "        self.handle.close()\r\n",
    "\r\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_length (__main__.FileTestCase) ... ok\n",
      "test_add_and_remove_product (__main__.ShoppingCartTestCase) ... ok\n",
      "test_transform_name (__main__.TestProduct) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fec12830b80>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assertions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Python has assertion statements by default that you can use in your code to make sure a condition is met. It works similar to an `if/else` statement, but in this case, if the condition is not fulfilled, the code will directly raise an error."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ivan = 'dumb'\r\n",
    "assert ivan == 'dumb' # The assertion doesn't raise an error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "assert ivan == 'smart' # The assertion raises an error. I guess I should study more"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ecf73eaeb41a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mivan\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'smart'\u001b[0m \u001b[0;31m# The assertion raises an error. I guess I should study more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "unittest has its own methods for creating these assertions, and that way, when running a test, it will add up each failed assertion to a total. Examples of these assertions are:\n",
    "\n",
    "- `assertEqual(a, b)`\n",
    "- `assertNotEqual(a, b)`\n",
    "- `assertTrue(x)`\n",
    "- `assertFalse(x)`\n",
    "- `assertIsInstance(a, b)`\n",
    "- `assertIn(a, b)`\n",
    "- `assertAlmostEqual(a, b)` like `round(a-b, 7) == 0`\n",
    "- `assertGreaterEqual(a, b)`\n",
    "- `assertRegex(s, r)` like `r.search(s)`\n",
    "- `assertMultiLineEqual(a, b)` compares two strings\n",
    "- `assertDictEqual(a, b)` compares two dictionaries\n",
    "- `assertSequenceEqual(a, b)` compares two sequences\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at this page for a more [exhaustive list](https://docs.python.org/3/library/unittest.html#module-unittest)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write your first test suite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `product.py` file was fairly simple, so let's spice things up. We are going to add these products to a Shopping Cart, so let's create a new module for said Cart. Thanks to the magic of preparing lessons, the package `Cart` already has a `cart.py` module. The ShoppingCart class has two methods: `add_product`, and `remove_product`. The product(s) we will add are products from the Product class.\n",
    "\n",
    "So, if everything is fine, we should be able to add a product, and after removing, the cart will be empty."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import unittest\r\n",
    "from example.cart import ShoppingCart\r\n",
    "from example.product import Product\r\n",
    "\r\n",
    "class ShoppingCartTestCase(unittest.TestCase):\r\n",
    "    def test_add_and_remove_product(self):\r\n",
    "        cart = ShoppingCart()\r\n",
    "        product = Product('Polo', 'S', 'Navy Blue')\r\n",
    "        \r\n",
    "        cart.add_product(product)\r\n",
    "        cart.remove_product(product)\r\n",
    "        # Check if the products attribute is empty\r\n",
    "        # The assertDictEqual check if two dicts are equal\r\n",
    "        self.assertDictEqual({}, cart.products) \r\n",
    "\r\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_add_and_remove_product (__main__.ShoppingCartTestCase) ... ok\n",
      "test_transform_name (__main__.TestProduct) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.037s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fec11a19fd0>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks great! Something that you might have noticed is that it ran three tests, but we just specified one. This is because once you run a test, that Test class will be stored if you are working in a notebook, or they will all be ran if you are working in the command line. \n",
    "\n",
    "This is actually one of the beauties of unittest, as you start writing tests, they will be added to your _test suite_, so next time you change your code, you can simply run the _test suite_ to check all functions at once."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing that you might not notice is that we are already implementing integration testing, the second level of granularity in the pyramid testing. Observe that `test_add_and_remove_product` calls for both `generate_sku` from `product.py`, and `add_product` and `remove_product` from `cart.py`. You will have the chance to add more tests to the test suite during the challenges."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Skipping tests\n",
    "\n",
    "We can skip certain tests given specific consitions using decorators:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "\r\n",
    "class MyTestCase(unittest.TestCase):\r\n",
    "    @unittest.skip(\"demonstrating skipping\")\r\n",
    "    def test_nothing(self):\r\n",
    "        # This line will not run at all\r\n",
    "        self.fail(\"shouldn't happen\")\r\n",
    "\r\n",
    "    @unittest.skipIf(sys.version_info.major < 3, \"Not supported for Python 2\")\r\n",
    "    def test_py3_format(self):\r\n",
    "        self.assertEqual(\"{}\".format(\"aaa\"), \"aaa\")\r\n",
    "        pass\r\n",
    "\r\n",
    "    @unittest.skipUnless(sys.platform.startswith(\"win\"), \"Windows required\")\r\n",
    "    def test_windows_support(self):\r\n",
    "        # windows specific testing code\r\n",
    "        pass\r\n",
    "\r\n",
    "    def test_maybe_skipped(self):\r\n",
    "        # Skip test from within the function body\r\n",
    "        if 5 == 5:\r\n",
    "            self.skipTest(\"Yes, 5 is equal to 5 so we skip\")\r\n",
    "        # test code which would run if 5 != 5 (essentially never, we know)\r\n",
    "        ...\r\n",
    "        \r\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hypothesis\n",
    "\n",
    "> __Hypothesis is a `python` library for `property-based testing` which is easier, more powerful and has way larger test cases coverage than standard unit testing__\n",
    "\n",
    "Difference between `unit testing` and `property-based testing` is fantastically explained by [Hypothesis Welcome Page](https://hypothesis.readthedocs.io/en/latest/):\n",
    "\n",
    "__Think of a normal unit test as being something like the following:__\n",
    "\n",
    "1. Set up some data.\n",
    "2. Perform some operations on the data.\n",
    "3. Assert something about the result.\n",
    "\n",
    "__Hypothesis lets you write tests which instead look like this:__\n",
    "\n",
    "1. For all data matching some specification.\n",
    "2. Perform some operations on the data.\n",
    "3. Assert something about the result.\n",
    "\n",
    "This idea was popularized by [Haskell](https://www.haskell.org/) (purely functional programming language) library [QuickCheck](https://hackage.haskell.org/package/QuickCheck).\n",
    "\n",
    "> __Hypothesis generates testing data based on your specification and checks whether guarantees you want to give hold true.__\n",
    "\n",
    "### Installation\n",
    "\n",
    "As per usual, one can install `hypothesis` via `pip` or [`conda`](https://anaconda.org/conda-forge/hypothesis).\n",
    "\n",
    "There are also a few extensions provided for scientific stack especially (like `numpy` or `pandas`).\n",
    "\n",
    "To install `hypothesis` with `numpy` generation strategies via `pip` one could do "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General\n",
    "\n",
    "First, let's set up an example which `encode`s the string and `decode`s it:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def encode(input_string):\r\n",
    "    count = 1\r\n",
    "    prev = \"\"\r\n",
    "    lst = []\r\n",
    "    for character in input_string:\r\n",
    "        if character != prev:\r\n",
    "            if prev:\r\n",
    "                entry = (prev, count)\r\n",
    "                lst.append(entry)\r\n",
    "            count = 1\r\n",
    "            prev = character\r\n",
    "        else:\r\n",
    "            count += 1\r\n",
    "    entry = (character, count)\r\n",
    "    lst.append(entry)\r\n",
    "    return lst\r\n",
    "\r\n",
    "\r\n",
    "def decode(lst):\r\n",
    "    q = \"\"\r\n",
    "    for character, count in lst:\r\n",
    "        q += character * count\r\n",
    "    return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It should be fairly obvious, that `encode(decode(<string>))` should return original `<string>`.\n",
    "\n",
    "Hypothesis can generate `<string>` examples for us (just like unit tests, but way easier and automated) using:\n",
    "- `strategy` - way to create testing data (in this case `text`)\n",
    "- `given` - generate samples from the specified strategy\n",
    "\n",
    "With that in mind, let's see how we could do that:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Change that to unittest\r\n",
    "\r\n",
    "from hypothesis import given\r\n",
    "import hypothesis.strategies as st\r\n",
    "\r\n",
    "class TestEncoding(unittest.TestCase):\r\n",
    "    @given(st.text())\r\n",
    "    def test_decode_inverts_encode(self, s):\r\n",
    "        self.assertEqual(decode(encode(s)), s)\r\n",
    "        \r\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all notice how easy it is to mix `unittest` with `hypothesis` to create way more comprehensive test suite.\n",
    "\n",
    "You can see that for our string encoding function fails when the input is an empty string. If we fix the above code by appending\n",
    "the check for empty string:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def encode(input_string):\r\n",
    "    # This is an example fix\r\n",
    "    if input_string == \"\":\r\n",
    "        return []\r\n",
    "    \r\n",
    "    count = 1\r\n",
    "    prev = \"\"\r\n",
    "    lst = []\r\n",
    "    for character in input_string:\r\n",
    "        if character != prev:\r\n",
    "            if prev:\r\n",
    "                entry = (prev, count)\r\n",
    "                lst.append(entry)\r\n",
    "            count = 1\r\n",
    "            prev = character\r\n",
    "        else:\r\n",
    "            count += 1\r\n",
    "    entry = (character, count)\r\n",
    "    lst.append(entry)\r\n",
    "    return lst\r\n",
    "\r\n",
    "\r\n",
    "def decode(lst):\r\n",
    "    q = \"\"\r\n",
    "    for character, count in lst:\r\n",
    "        q += character * count\r\n",
    "    return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And re-running the test (`@example` specifies this example will always be run, good for catching edge cases):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generated tests pass correctly.\n",
    "\n",
    "> __Hypothesis is smart about running tests, IT WILL ONLY RUN THE FAILED CASES (as it has it's own internal database)!__\n",
    "\n",
    "### Hypothesis tricks\n",
    "\n",
    "A few useful things, which should help you with your tests:\n",
    "\n",
    "> __`filter` values generated by a strategy__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "@given(st.integers().filter(lambda x: x % 2 == 0))\r\n",
    "def test_even_integers(i):\r\n",
    "    pass\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> __`assume` that input is/is not something__\n",
    "\n",
    "> __NOTE:__ Hypothesis will fail if your assumptions get rid of too many generated samples\n",
    "\n",
    "Test will not be marked as `failing` __if the assumption is `false`__:\n",
    "from math import isnan"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@given(st.floats())\r\n",
    "def test_negation_is_self_inverse_for_non_nan(x):\r\n",
    "    assume(not isnan(x))\r\n",
    "    assert x == -(-x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> __strategies are highly customizable__\n",
    "\n",
    "One can specify a lot of parameters for the strategies, for example:\n",
    "st.integers(min_value=0, max_value=10).example()\n",
    "> __`given` can specify some/all argument to function via `kwargs` or `args`__\n",
    "\n",
    "With the following signature:\n",
    "\n",
    "```\n",
    "hypothesis.given(*_given_arguments, **_given_kwargs)\n",
    "```\n",
    "\n",
    "Valid cases could be (amongst others):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@given(st.integers(), st.integers())\r\n",
    "def a(x, y):\r\n",
    "    pass\r\n",
    "\r\n",
    "\r\n",
    "@given(st.integers())\r\n",
    "def b(x, y):\r\n",
    "    pass\r\n",
    "\r\n",
    "\r\n",
    "@given(y=st.integers())\r\n",
    "def c(x, y):\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "- Testing your code is crucial as your software starts growing\n",
    "- Good test suites are key for a continuous integration (CI) environment\n",
    "- Non-Functional testing checks parameters such as performance and security\n",
    "- Functional testing checks that your code works as intended with no issues\n",
    "- Unit testing is the lowest level of granularity in your testing schema\n",
    "- You can use `unittest` to create unit testing and integration testing\n",
    "- As you create more test classes, the test suite will grow bigger\n",
    "- Instead of giving a whole range of values, you can use the `hypothesis` library to set a strategy for a test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Challenges\r\n",
    "\r\n",
    "### Q1. The Stock Keeping Unit has to be updated, so if you add products or colours with whitespaces, the SKU removes them:\r\n",
    "`'tank top', 'm', 'navy blue'` should look `'TANKTOP-M-NAVYBLUE'`\r\n",
    "### Q2. The Stock Keeping Unit has to be updated, so if you add products with dashes, the SKU removes them:\r\n",
    "`'tshirt', 'm', 'navy blue'` should look `'TSHIRT-M-NAVYBLUE'`\r\n",
    "### Q3. Run the test_product you created during the lesson. Modify it so you don't get any failed test\r\n",
    "### Q4. Add the following tests to the test suite:\r\n",
    "#### $\\qquad$ 1. `test_cart_initially_empty()`: tests that, after creating the cart instance, it starts as an empty dictionary\r\n",
    "#### $\\qquad$ 2. `test_add_prodcut()`: tests that, after adding a product to the cart, cart.products will be equal to a dictionary like this: `{'SHOES-S-BLUE': {'quantity': 1}}`\r\n",
    "#### $\\qquad$ 3. `test_add_two_of_a_product`: tests that, after adding a product passing the argument `quantity` equals 2, cart.product has a dictionary whose quantity is 2\r\n",
    "#### $\\qquad$ 4. `test_add_two_different_products`\r\n",
    "### Q5. Add the following test to the test suite:\r\n",
    "```\r\n",
    "def test_remove_too_many(self):\r\n",
    "    cart = ShoppingCart()\r\n",
    "    product = Product('shoes', 'S', 'blue')\r\n",
    "    \r\n",
    "    cart.add_product(product)\r\n",
    "    cart.remove_product(product, quantity=2)\r\n",
    "\r\n",
    "    self.assertDictEqual({}, cart.products)\r\n",
    "```\r\n",
    "#### Does it run fine? How can you fix it?\r\n",
    "\r\n",
    "### Q6. Decorate with hypothesis at least two of the tests in your test suite\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assessments:\n",
    "\n",
    "### 1. Look for Behaviour-Driven Development\n",
    "Take a look at [Cucumber.io](https://cucumber.io) \n",
    "### 2. Learn basics of `pytest` (documentation [here](https://docs.pytest.org/en/latest/contents.html))\n",
    "### 3. What is `pytest`'s [mark.parametrize](https://docs.pytest.org/en/stable/parametrize.html)? \n",
    "### 4. What is [Test Driven Development](https://en.wikipedia.org/wiki/Test-driven_development) and what are the general steps needed to follow this approach?\n",
    "### 5. What is [Unittest Mocking](https://docs.python.org/3/library/unittest.mock.html) and what are the general steps needed to follow this approach?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}